The goal of this project is to detect persons of interest (POI) in the Enron dataset. 
The dataset contains financial information about Enron employees and e-mails sent between them. This data can be used to train a machine learning algorithm that can classify people as POI or non-POI. 
We removed two data points, "TOTAL" and "THE TRAVEL AGENCY IN THE PARK", because they seemed irrelevant to the question we are trying to answer. 
We removed "email_address" from the feature list because the values are not numerical and do not add any additional information.
We also removed "loan_advances" because very few persons had a value for this feature and results seemed better not using it. 
After removing those two data points and those two features, the dataset contains 18 POIs, 126 non-POIs and 18 financial features.


To select which features to use, we used SelectKBest with f_regression as a score function. We also tried chi2 as a score function but that gave worse results.

Features sorted by f_regression score:
exercised_stock_options 24.8150797332
total_stock_value 24.1828986786
bonus 20.7922520472
salary 18.2896840434
deferred_income 11.4584765793
long_term_incentive 9.92218601319
restricted_stock 9.21281062198
total_payments 8.77277773009
shared_receipt_with_poi 8.58942073168
expenses 6.09417331064
from_poi_to_this_person 5.24344971337
other 4.187477507
from_this_person_to_poi 2.38261210823
director_fees 2.12632780201
to_messages 1.64634112944
deferral_payments 0.224611274736
from_messages 0.169700947622
restricted_stock_deferred 0.0654996529099

We selected k features with the highest f_regression score. We experimented with different values of k and k = 8 gave us the best result.

We used MinMaxScaler to scale the features, both before the feature selection and in the classifier pipeline because the selected features had very different ranges.

Although we didn't engineer a new feature we used PCA to transform our feature space. We believe that having an orthogonal feature space (independent features) would be advantageous during training and classification.

We tested decision tree, support vector machines and AdaBoost, but nothing had better results than Naive Bayes. Adding PCA to the pipeline before the GaussianNB increased the F1 score from 0.34711 to 0.41518. We used PCA just to transform the features, and used as many components as we had features.

When we tested SVM, it did not work at all at first with default parameter values, but after increasing the C value we got some results. For AdaBoost we used GridSearchCV to test different number of trees and minimum number of samples for split, but the best resuls from that was still worse than Naive Bayes.


Validation is the process of running a trained classifier on a separate test set. A common mistake is to validate on the training set, which can lead to overfit. Another issue can be to not randomize the split of classes in the dataset as having an unbalanced set can skew performance figures. In the tester StratifiedShuffleSplit is used due to the small amount of data points in the dataset.

Some evaluation metrics we got are:
Precision: 0.45325	
Recall: 0.38300	
F1: 0.41518
Those mean that we can find 38% POIs out of all POIs, and of all the POIs that we classify, 45% are actually POIs. F1 combined precision and recall in a single term.







